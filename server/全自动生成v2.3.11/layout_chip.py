import json
from collections import defaultdict
from typing import List, Dict, Any, Tuple, Set

# --- å¸ƒå±€é…ç½® ---
# æ‚¨å¯ä»¥æ ¹æ®æœ€ç»ˆæ•ˆæœå¾®è°ƒè¿™äº›å€¼
X_SPACING = 800.0  # èŠ‚ç‚¹â€œåˆ—â€ä¹‹é—´çš„æ°´å¹³è·ç¦»
Y_SPACING = 600.0  # åŒä¸€åˆ—ä¸­èŠ‚ç‚¹ä¹‹é—´çš„æœ€å°å‚ç›´è·ç¦»
GLOBAL_X_OFFSET = -2000.0 # æ•´ä½“å‘å·¦å¹³ç§»ï¼Œä»¥é€‚åº”ç”»å¸ƒ

# --- æ–‡ä»¶å (ä»…åœ¨ç‹¬ç«‹è¿è¡Œæ—¶ä½¿ç”¨) ---
INPUT_FILENAME = 'ungraph.json'
OUTPUT_FILENAME = 'ungraph_layouted_ultimate.json'


# --- å…¨æ–°çš„â€œALAP + è´¨å¿ƒè¿­ä»£â€å¸ƒå±€å¼•æ“ ---

def parse_graph(nodes: List[Dict[str, Any]]) -> tuple:
    """è§£æèŠ‚ç‚¹åˆ—è¡¨ï¼Œæ„å»ºå¸ƒå±€æ‰€éœ€çš„æ•°æ®ç»“æ„ã€‚"""
    predecessors = defaultdict(list)
    successors = defaultdict(list)
    node_map = {node['Id']: node for node in nodes}

    for node_id, node in node_map.items():
        for input_port in node.get('Inputs', []):
            connection = input_port.get('connectedOutputIdModel')
            if connection and 'NodeId' in connection:
                source_id = connection['NodeId']
                if source_id in node_map:
                    successors[source_id].append(node_id)
                    predecessors[node_id].append(source_id)
    return predecessors, successors, list(node_map.keys())

def calculate_alap_layers(node_ids: list, predecessors: dict, successors: dict) -> dict:
    """
    æ ¸å¿ƒå‡çº§ï¼šä½¿ç”¨ ALAP (As Late As Possible) ç®—æ³•åˆ†å±‚ã€‚
    è¿™ä¼šå°†èŠ‚ç‚¹å°½å¯èƒ½åœ°å‘å³æ¨ï¼Œé¿å…åœ¨ç¬¬ä¸€å±‚å †ç§¯ã€‚
    """
    # 1. å…ˆè¿›è¡Œä¸€æ¬¡ASAPï¼ˆä»å·¦åˆ°å³ï¼‰åˆ†å±‚ï¼Œä»¥ç¡®å®šå›¾çš„æ€»æ·±åº¦
    asap_layers = {}
    q = [n for n in node_ids if not predecessors[n]]
    for node_id in q: asap_layers[node_id] = 0
    
    head = 0
    processed_in_asap = set(q)
    while head < len(q):
        u = q[head]; head += 1
        for v in successors[u]:
            if v not in asap_layers:
                asap_layers[v] = 0
            asap_layers[v] = max(asap_layers[v], asap_layers[u] + 1)
            if v not in processed_in_asap:
                q.append(v)
                processed_in_asap.add(v)
    
    max_layer = max(asap_layers.values()) if asap_layers else 0

    # 2. è¿›è¡ŒALAPï¼ˆä»å³åˆ°å·¦ï¼‰åˆ†å±‚
    alap_layers = {}
    q = [n for n in node_ids if not successors[n]]
    for node_id in q: alap_layers[node_id] = max_layer
    
    head = 0
    processed_in_alap = set(q)
    while head < len(q):
        u = q[head]; head += 1
        for v in predecessors[u]:
            if v not in alap_layers:
                alap_layers[v] = max_layer
            alap_layers[v] = min(alap_layers[v], alap_layers[u] - 1)
            if v not in processed_in_alap:
                q.append(v)
                processed_in_alap.add(v)
            
    # å°†å±‚ä¿¡æ¯ç»„ç»‡æˆå­—å…¸
    layers_map = defaultdict(list)
    for node_id, layer in alap_layers.items():
        layers_map[layer].append(node_id)
        
    return layers_map

# â€”â€” å°†è·¨å¤šåˆ—çš„è¾¹"è™šæ‹Ÿæ‹†è¾¹"ä¸ºç›¸é‚»åˆ—è¾¹ï¼Œå¹¶æ„å»ºä»…ç›¸é‚»åˆ—çš„å‰é©±/åç»§æ˜ å°„ â€”â€”
def _insert_dummies_and_build_adj(predecessors, successors, col_node, cols):
    cols_aug = {c: list(arr) for c, arr in cols.items()}  # æ¯åˆ—åŒ…å«çœŸå®èŠ‚ç‚¹ï¼Œåç»­ä¼šåŠ å…¥ dummy
    pred_adj = defaultdict(list)  # ä»…ç›¸é‚»åˆ—çš„å‰é©±ï¼ˆå« dummyï¼‰
    succ_adj = defaultdict(list)
    dummy_id = 0

    def new_dummy():
        nonlocal dummy_id
        dummy_id += 1
        return f"__DUMMY__{dummy_id}"

    # éå†æ¯æ¡æœ‰å‘è¾¹ï¼ŒæŒ‰åˆ—å·®é€æ­¥"èµ°è¿‡å»"
    for u, outs in successors.items():
        cu = col_node.get(u)
        if cu is None: 
            continue
        for v in outs:
            cv = col_node.get(v)
            if cv is None or cv == cu: 
                continue
            step = 1 if cv > cu else -1
            prev = u
            for c in range(cu + step, cv + step, step):
                if c == cv:
                    succ_adj[prev].append(v)
                    pred_adj[v].append(prev)
                else:
                    d = new_dummy()
                    cols_aug.setdefault(c, []).append(d)
                    col_node[d] = c
                    succ_adj[prev].append(d)
                    pred_adj[d].append(prev)
                    prev = d
    return cols_aug, pred_adj, succ_adj

def _bary_sweeps_with_dummies(cols_aug, col_node, pred_adj, succ_adj, passes=4):
    # å¤šè½®ï¼šLeftâ†’Right ç”¨å·¦é‚»åˆ—çš„ä¸­ä½ç§©ï¼›Rightâ†’Left ç”¨å³é‚»åˆ—çš„ä¸­ä½ç§©
    def order_maps(cols):
        return {c: {nid: i for i, nid in enumerate(cols.get(c, []))} for c in cols}

    col_keys = sorted(cols_aug.keys())
    for _ in range(passes):
        om = order_maps(cols_aug)
        # ---- L â†’ R ----
        for c in col_keys[1:]:
            arr = cols_aug.get(c, [])
            if not arr: 
                continue
            left = om.get(c - 1, {})
            def bary_left(nid):
                idxs = [left[x] for x in pred_adj.get(nid, []) if x in left]
                return (sum(idxs) / len(idxs)) if idxs else om[c].get(nid, 0)
            arr.sort(key=lambda nid: (bary_left(nid), om[c][nid]))
            om[c] = {nid: i for i, nid in enumerate(arr)}
        # ---- R â†’ L ----
        om = order_maps(cols_aug)
        for c in reversed(col_keys[:-1]):
            arr = cols_aug.get(c, [])
            if not arr: 
                continue
            right = om.get(c + 1, {})
            def bary_right(nid):
                idxs = [right[x] for x in succ_adj.get(nid, []) if x in right]
                return (sum(idxs) / len(idxs)) if idxs else om[c].get(nid, 0)
            arr.sort(key=lambda nid: (bary_right(nid), om[c][nid]))
            om[c] = {nid: i for i, nid in enumerate(arr)}

def iterative_barycenter_positioning(layers: dict, predecessors: dict, successors: dict) -> dict:
    """
    æ ¸å¿ƒå‡çº§ï¼šä½¿ç”¨è™šæ‹Ÿæ‹†è¾¹å’ŒåŒå‘å¤šè½®ä¸­ä½æ•°æ‰«æ ä¼˜åŒ–å‚ç›´ä½ç½®ï¼Œä»¥æœ€å¤§ç¨‹åº¦å‡å°‘çº¿æ¡äº¤å‰ã€‚
    """
    positions = {}
    
    # 1) åˆ— â†’ èŠ‚ç‚¹
    cols: Dict[int, List[str]] = defaultdict(list)
    col_node = {}
    for layer, nodes in layers.items():
        for node_id in nodes:
            cols[layer].append(node_id)
            col_node[node_id] = layer
    for c in cols:
        cols[c].sort()  # åˆå§‹ç¨³å®šåº

    # 2) å…ˆ"è™šæ‹Ÿæ‹†è¾¹"ä¸ºç›¸é‚»åˆ—è¾¹ï¼Œå†åšåŒå‘å¤šè½®ä¸­ä½æ•°æ‰«æ ï¼Œå¾—åˆ°æ›´å¥½çš„åˆ—å†…é¡ºåº
    col_node_aug = dict(col_node)  # ä¼šåŠ  dummy çš„åˆ—å·
    cols_aug, pred_adj, succ_adj = _insert_dummies_and_build_adj(
        predecessors, successors, col_node_aug, cols
    )
    _bary_sweeps_with_dummies(cols_aug, col_node_aug, pred_adj, succ_adj, passes=4)

    # 3) æ‰«æ å®Œæˆåï¼Œåªå¯¹"çœŸå®èŠ‚ç‚¹"èµ‹ yï¼ˆdummy ä»…å‚ä¸æ’åºï¼Œä¸è¾“å‡ºåæ ‡ï¼‰
    y_order: Dict[str, float] = {}
    for c in sorted(cols_aug.keys()):
        real_nodes = [nid for nid in cols_aug[c] if not str(nid).startswith("__DUMMY__")]
        for idx, nid in enumerate(real_nodes):
            y_order[nid] = idx * Y_SPACING

    # è½¬æ¢ä¸ºè¿”å›æ ¼å¼
    for node_id, y in y_order.items():
        positions[node_id] = {'y': y}

    return positions

# -------------------------------------------------------------
# äº¤æ›¿æ‰«æ çš„åŠ æƒé‡å¿ƒæ¶ˆäº¤å‰ï¼ˆä¸æ’ dummyï¼ŒæŒ‰åˆ—è·ç¦» 1/Î”col è¡°å‡ï¼‰
# -------------------------------------------------------------
def _rebuild_order_maps(cols: Dict[int, List[str]]) -> Dict[int, Dict[str, int]]:
    return {c: {nid: i for i, nid in enumerate(cols.get(c, []))} for c in cols}

def _weighted_bary_wrt_side(n: str, side: str,
                            col_of: Dict[str, int],
                            order_maps: Dict[int, Dict[str, int]],
                            predecessors: Dict[str, List[str]],
                            successors: Dict[str, List[str]]) -> float:
    cn = col_of[n]
    if side == "left":
        neis = [p for p in predecessors.get(n, []) if p in col_of and col_of[p] < cn]
        if not neis:
            return order_maps[cn].get(n, 0)
        s = wsum = 0.0
        for p in neis:
            cp = col_of[p]
            idx = order_maps.get(cp, {}).get(p)
            if idx is None: 
                continue
            w = 1.0 / (cn - cp)  # è¶Šè¿‘æƒé‡è¶Šå¤§
            s += w * idx; wsum += w
        return s / wsum if wsum > 0 else order_maps[cn][n]
    else:  # "right"
        neis = [q for q in successors.get(n, []) if q in col_of and col_of[q] > cn]
        if not neis:
            return order_maps[cn].get(n, 0)
        s = wsum = 0.0
        for q in neis:
            cq = col_of[q]
            idx = order_maps.get(cq, {}).get(q)
            if idx is None:
                continue
            w = 1.0 / (cq - cn)
            s += w * idx; wsum += w
        return s / wsum if wsum > 0 else order_maps[cn][n]

def _minimize_crossings_by_sweeps(cols: Dict[int, List[str]],
                                  col_of: Dict[str, int],
                                  predecessors: Dict[str, List[str]],
                                  successors: Dict[str, List[str]],
                                  passes: int = 4) -> None:
    if not cols:
        return
    col_keys = sorted(cols.keys())
    for _ in range(passes):
        # Left -> Rightï¼šçœ‹å·¦ä¾§é‚»å±…
        order_maps = _rebuild_order_maps(cols)
        for c in col_keys[1:]:
            if c not in cols or not cols[c]: 
                continue
            arr = cols[c]
            arr.sort(key=lambda nid: (_weighted_bary_wrt_side(nid, "left", col_of, order_maps, predecessors, successors),
                                      order_maps[c][nid]))
            # æ›´æ–°è¯¥åˆ—çš„æ˜ å°„ï¼Œä¾¿äºåŒè½®åé¢çš„åˆ—ä½¿ç”¨æ›´å‡†ç¡®çš„ä½ç½®
            order_maps[c] = {nid: i for i, nid in enumerate(arr)}

        # Right -> Leftï¼šçœ‹å³ä¾§é‚»å±…
        order_maps = _rebuild_order_maps(cols)
        for c in reversed(col_keys[:-1]):
            if c not in cols or not cols[c]:
                continue
            arr = cols[c]
            arr.sort(key=lambda nid: (_weighted_bary_wrt_side(nid, "right", col_of, order_maps, predecessors, successors),
                                      order_maps[c][nid]))
            order_maps[c] = {nid: i for i, nid in enumerate(arr)}

def resolve_overlaps_and_finalize(layers: dict, temp_positions: dict) -> dict:
    """æœ€åä¸€æ­¥ï¼šè§£å†³é‡å ï¼Œå¹¶æœ€ç»ˆç¡®å®šX,Yåæ ‡ã€‚"""
    final_positions = {}
    for i in sorted(layers.keys()):
        # è¿‡æ»¤æ‰ä¸åœ¨ temp_positions ä¸­çš„èŠ‚ç‚¹ï¼Œä»¥é˜²ä¸‡ä¸€
        nodes_in_layer = sorted(
            [n for n in layers[i] if n in temp_positions],
            key=lambda n: temp_positions[n]['y']
        )
        
        # è§£å†³é‡å 
        for j in range(1, len(nodes_in_layer)):
            prev_node, curr_node = nodes_in_layer[j-1], nodes_in_layer[j]
            min_y = temp_positions[prev_node]['y'] + Y_SPACING
            if temp_positions[curr_node]['y'] < min_y:
                temp_positions[curr_node]['y'] = min_y
                
        # åˆ†é…æœ€ç»ˆåæ ‡
        for node_id in nodes_in_layer:
            final_positions[node_id] = {'x': i * X_SPACING, 'y': temp_positions[node_id]['y']}
            
    # å‚ç›´å±…ä¸­æ•´ä¸ªå¸ƒå±€
    all_ys = [pos['y'] for pos in final_positions.values()]
    if all_ys:
        center_offset = (min(all_ys) + max(all_ys)) / 2.0
        for node_id in final_positions:
            final_positions[node_id]['y'] -= center_offset
            
    return final_positions


def find_and_update_chip_graph(data: dict, final_positions: dict) -> bool:
    """åœ¨JSONä¸­æ‰¾åˆ°èŠ¯ç‰‡å›¾æ•°æ®å¹¶æ›´æ–°èŠ‚ç‚¹åæ ‡ã€‚"""
    try:
        # è·¯å¾„å¯èƒ½å› å­˜æ¡£ç»“æ„è€Œå¼‚ï¼Œè¿™é‡Œå‡è®¾æ˜¯æ ‡å‡†ç»“æ„
        save_obj = data['saveObjectContainers'][0]['saveObjects']
        for meta_data in save_obj['saveMetaDatas']:
            if meta_data.get('key') == 'chip_graph':
                graph_data = json.loads(meta_data['stringValue'])
                nodes_updated = 0
                for node in graph_data.get('Nodes', []):
                    if node['Id'] in final_positions:
                        pos = final_positions[node['Id']]
                        node['VisualPosition']['x'] = pos['x'] + GLOBAL_X_OFFSET
                        node['VisualPosition']['y'] = pos['y']
                        nodes_updated += 1
                
                if nodes_updated > 0:
                    meta_data['stringValue'] = json.dumps(graph_data, separators=(',', ':'))
                    print(f"   åœ¨'chip_graph'ä¸­æ›´æ–°äº† {nodes_updated} ä¸ªèŠ‚ç‚¹çš„ä½ç½®ã€‚")
                    return True
        print("   è­¦å‘Š: åœ¨JSONä¸­æ‰¾åˆ°äº†'chip_graph'ï¼Œä½†æ²¡æœ‰éœ€è¦æ›´æ–°åæ ‡çš„åŒ¹é…èŠ‚ç‚¹ã€‚")
        return False
    except (KeyError, IndexError, TypeError) as e:
        print(f"é”™è¯¯ï¼šå¯¼èˆªJSONç»“æ„æ—¶å‡ºé”™: {e}ã€‚è¯·æ£€æŸ¥å­˜æ¡£æ–‡ä»¶ç»“æ„æ˜¯å¦æ­£ç¡®ã€‚")
        return False

# -------------------------------------------------------------
# é±¼ç¾¤å¼å±€éƒ¨äº¤æ¢é˜¶æ®µï¼ˆPairwise Swap, at most once per pairï¼‰
# -------------------------------------------------------------

def _build_cluster_columns_for_positions(cluster: List[str],
                                         final_positions: Dict[str, Dict[str, float]]
                                         ) -> Tuple[Dict[int, List[str]], Dict[str, int], Dict[int, Dict[str, int]]]:
    """
    ä¾æ®æœ€ç»ˆåæ ‡æŠŠ cluster å†…èŠ‚ç‚¹åˆ†åˆ°â€œåˆ—â€ï¼ˆcolumnï¼‰é‡Œï¼Œå¹¶å»ºç«‹åˆ—å†…çš„ rankï¼ˆæŒ‰ y ä»å°åˆ°å¤§ï¼‰ã€‚
    - åˆ—å·æŒ‰ï¼šcol = round((x - min_x_cluster) / X_SPACING)
    è¿”å›ï¼š
      cols: {col -> [nodes ordered by y]}
      col_of: {node -> col}
      rank_maps: {col -> {node -> rank_index}}
    """
    xs = [final_positions[n]['x'] for n in cluster if n in final_positions]
    if not xs:
        return {}, {}, {}
    min_x = min(xs)
    cols: Dict[int, List[str]] = defaultdict(list)
    col_of: Dict[str, int] = {}

    for n in cluster:
        if n not in final_positions:
            continue
        x = final_positions[n]['x']
        c = int(round((x - min_x) / X_SPACING))
        cols[c].append(n)
        col_of[n] = c

    # æ¯åˆ—æŒ‰ y æ’åºå¹¶å»ºç«‹ rank
    rank_maps: Dict[int, Dict[str, int]] = {}
    for c, arr in cols.items():
        arr.sort(key=lambda nid: final_positions[nid]['y'])
        rank_maps[c] = {nid: i for i, nid in enumerate(arr)}
    return cols, col_of, rank_maps


def _count_inversions(A: List[str], B: List[str], rank_map: Dict[str, int]) -> int:
    """ç»Ÿè®¡é›†åˆ A çš„ç«¯ç‚¹æ˜¯å¦â€œåœ¨ rank ä¸Šæ–¹äºâ€é›†åˆ B çš„ç«¯ç‚¹ï¼ˆra > rbï¼‰â†’ è¡¨ç¤ºå­˜åœ¨äº¤å‰ã€‚"""
    inv = 0
    for a in A:
        ra = rank_map.get(a)
        if ra is None: 
            continue
        for b in B:
            rb = rank_map.get(b)
            if rb is None:
                continue
            if ra > rb:
                inv += 1
    return inv


def _median_or_bary_rank(neis: List[str], rank_map: Dict[str, int]) -> float | None:
    vals = [rank_map[n] for n in neis if n in rank_map]
    if not vals:
        return None
    vals.sort()
    m = len(vals)
    if m % 2:
        return float(vals[m // 2])
    return 0.5 * (vals[m // 2 - 1] + vals[m // 2])


def _score_delta_if_swap(u: str, v: str, cur_col: int,
                         predecessors: Dict[str, List[str]],
                         successors: Dict[str, List[str]],
                         col_of: Dict[str, int],
                         cols: Dict[int, List[str]],
                         rank_maps: Dict[int, Dict[str, int]],
                         w_c: float = 1.0, w_m: float = 0.5) -> float:
    """
    åªæ¯”è¾ƒä¸ u,v ç›¸å…³çš„è¾¹ä¸ç§©ï¼šäº¤å‰é¡¹ + é‡å¿ƒé¡¹ã€‚
    è¿”å› Î”score = after - beforeï¼ˆè´Ÿè¡¨ç¤ºæ›´å¥½ï¼‰
    """
    # å½“å‰åˆ—å†… rank
    rank_cur = rank_maps[cur_col]
    pos_u, pos_v = rank_cur[u], rank_cur[v]

    # å·¦åˆ— / å³åˆ—çš„ rank
    rank_left  = rank_maps.get(cur_col - 1, {})
    rank_right = rank_maps.get(cur_col + 1, {})

    # ç›¸é‚»åˆ—ä¸­çš„é‚»å±…
    Lu = [p for p in predecessors.get(u, []) if col_of.get(p) == cur_col - 1]
    Lv = [p for p in predecessors.get(v, []) if col_of.get(p) == cur_col - 1]
    Ru = [s for s in successors.get(u, [])   if col_of.get(s) == cur_col + 1]
    Rv = [s for s in successors.get(v, [])   if col_of.get(s) == cur_col + 1]

    # äº¤å‰æ•°ï¼ˆå·¦å³ä¸¤ä¾§ç‹¬ç«‹ç»Ÿè®¡ï¼‰
    before_left  = _count_inversions(Lu, Lv, rank_left)
    after_left   = _count_inversions(Lv, Lu, rank_left)
    before_right = _count_inversions(Ru, Rv, rank_right)
    after_right  = _count_inversions(Rv, Ru, rank_right)

    cross_before = before_left + before_right
    cross_after  = after_left  + after_right

    # é‡å¿ƒï¼ˆæŠŠå·¦å³çš„ä¸­ä½ç§©åšå¹³å‡ï¼‰
    mu_left  = _median_or_bary_rank(Lu, rank_left)
    mv_left  = _median_or_bary_rank(Lv, rank_left)
    mu_right = _median_or_bary_rank(Ru, rank_right)
    mv_right = _median_or_bary_rank(Rv, rank_right)

    def bary_cost(pos, m1, m2):
        mm = [m for m in (m1, m2) if m is not None]
        if not mm:
            return 0.0
        avg = sum(mm) / len(mm)
        return abs(pos - avg)

    median_before = bary_cost(pos_u, mu_left, mu_right) + bary_cost(pos_v, mv_left, mv_right)
    median_after  = bary_cost(pos_v, mu_left, mu_right) + bary_cost(pos_u, mv_left, mv_right)

    score_before = w_c * cross_before + w_m * median_before
    score_after  = w_c * cross_after  + w_m * median_after
    return score_after - score_before


def _apply_swap_in_column(u: str, v: str, col: int,
                          cols: Dict[int, List[str]],
                          rank_maps: Dict[int, Dict[str, int]],
                          final_positions: Dict[str, Dict[str, float]]) -> None:
    """äº¤æ¢åŒåˆ—ä¸­çš„ç›¸é‚»ä¸¤ç‚¹ u,v ï¼ˆåªäº¤æ¢ yï¼‰ï¼Œç»´æŠ¤åˆ—æ•°ç»„ä¸ rank_map ä¸€è‡´æ€§ã€‚"""
    arr = cols[col]
    iu, iv = rank_maps[col][u], rank_maps[col][v]
    if iu > iv:
        iu, iv = iv, iu
        u, v = v, u

    # äº¤æ¢ y
    yu, yv = final_positions[u]['y'], final_positions[v]['y']
    final_positions[u]['y'], final_positions[v]['y'] = yv, yu

    # äº¤æ¢é¡ºåºåŠ rank
    arr[iu], arr[iv] = arr[iv], arr[iu]
    rank_maps[col][arr[iu]] = iu
    rank_maps[col][arr[iv]] = iv


def _fishschool_local_swaps(predecessors: Dict[str, List[str]],
                            successors: Dict[str, List[str]],
                            undirected: Dict[str, Set[str]],
                            clusters: List[List[str]],
                            final_positions: Dict[str, Dict[str, float]],
                            max_pass: int = 3) -> Dict[str, Dict[str, float]]:
    """
    åœ¨ç°æœ‰åæ ‡åŸºç¡€ä¸Šè¿›è¡Œâ€œåŒåˆ—ç›¸é‚»å¯¹â€çš„ä¸€æ¬¡æ€§äº¤æ¢å¯å‘å¼ï¼š
      - é€ cluster / é€åˆ—éå†ï¼›
      - å¯¹ç›¸é‚»å¯¹ (u,v) è‹¥ Î”score < 0 ä¸”æ­¤å¯¹æœªäº¤æ¢è¿‡ â†’ äº¤æ¢ï¼ŒåŠ å…¥é˜Ÿåˆ—çš„é‚»è¿‘å¯¹ç»§ç»­è¯„ä¼°ï¼›
      - åŒä¸€å¯¹åœ¨æ•´ä¸ªé˜¶æ®µè‡³å¤šäº¤æ¢ä¸€æ¬¡ï¼ˆé¿å…æŠ–åŠ¨ï¼‰ã€‚
    """
    swapped_once: Set[int] = set()  # ä»¥ hash_pair è®°å½•â€œå…¨å±€åªäº¤æ¢ä¸€æ¬¡â€
    def hash_pair(a: str, b: str) -> int:
        if a > b: a, b = b, a
        return (hash(a) << 1) ^ hash(b)

    for _ in range(max_pass):
        # éå†æ¯ä¸ª cluster
        for cluster in clusters:
            # åŸºäºæœ€ç»ˆåæ ‡é‡å»ºè¯¥ cluster çš„åˆ—ä¸ç§©
            cols, col_of, rank_maps = _build_cluster_columns_for_positions(cluster, final_positions)
            if not cols:
                continue

            # æ¯åˆ—åšâ€œå†’æ³¡å¼â€é‚»å¯¹è¯„ä¼°
            for c, arr in cols.items():
                if len(arr) <= 1:
                    continue
                # åˆå§‹åŒ–ç›¸é‚»å¯¹é˜Ÿåˆ—
                Q = [(i, i+1) for i in range(len(arr) - 1)]
                while Q:
                    i, j = Q.pop(0)
                    u, v = arr[i], arr[j]
                    key = hash_pair(u, v)
                    if key in swapped_once:
                        continue
                    delta = _score_delta_if_swap(u, v, c, predecessors, successors, col_of, cols, rank_maps)
                    if delta < 0:  # æ›´ä¼˜ â†’ äº¤æ¢ï¼Œå¹¶æ ‡è®°ä¸€æ¬¡æ€§
                        _apply_swap_in_column(u, v, c, cols, rank_maps, final_positions)
                        swapped_once.add(key)
                        # å—å½±å“çš„é‚»å¯¹å…¥é˜Ÿ
                        if i-1 >= 0: Q.append((i-1, i))
                        if j+1 < len(arr): Q.append((j, j+1))
    return final_positions

# --- æ–°å¢ï¼šå¯ä¾›å¤–éƒ¨è°ƒç”¨çš„ä¸»å‡½æ•° ---
def run_layout_engine(chip_nodes: List[Dict[str, Any]]) -> Dict[str, Dict[str, float]]:
    """
    æ¥æ”¶èŠ‚ç‚¹åˆ—è¡¨ï¼Œæ‰§è¡Œå®Œæ•´çš„å¸ƒå±€ç®—æ³•ï¼Œå¹¶è¿”å›æœ€ç»ˆä½ç½®ã€‚
    è¿™æ˜¯è¢« main.py è°ƒç”¨çš„æ ¸å¿ƒå…¥å£ã€‚
    """
    print("1. æ ¸å¿ƒæ­¥éª¤: æ‰§è¡Œ ALAP åˆ†å±‚...")
    predecessors, successors, node_ids = parse_graph(chip_nodes)
    layers = calculate_alap_layers(node_ids, predecessors, successors)
    print(f"   å®Œæˆã€‚å›¾è¢«åˆ†ä¸º {len(layers)} ä¸ªå±‚çº§ã€‚")

    print("2. æ ¸å¿ƒæ­¥éª¤: æ‰§è¡Œå¤šè½®è´¨å¿ƒè¿­ä»£...")
    temp_positions = iterative_barycenter_positioning(layers, predecessors, successors)
    print("   å®Œæˆã€‚")
    
    print("3. æœ€ç»ˆæ•´ç†: è§£å†³é‡å å¹¶å‚ç›´å±…ä¸­...")
    final_positions = resolve_overlaps_and_finalize(layers, temp_positions)
    print("   å®Œæˆ.")
    
    # === æ–°å¢ï¼šé±¼ç¾¤å¼å±€éƒ¨äº¤æ¢é˜¶æ®µï¼ˆåœ¨æ‰€æœ‰å¸ƒå±€é€»è¾‘ä¹‹åï¼‰ ===
    print("4. å±€éƒ¨äº¤æ¢ä¼˜åŒ–ï¼ˆé±¼ç¾¤å¼ï¼‰ â€¦")
    # ä¸ºäº†ä¸ç°æœ‰ä»£ç å…¼å®¹ï¼Œæˆ‘ä»¬æ„é€ ä¸€äº›å¿…è¦çš„å‚æ•°
    # æ³¨æ„ï¼šè¿™é‡Œçš„ 'undirected' å’Œ 'clusters' æ˜¯ç®€åŒ–å¤„ç†çš„ï¼Œå¯èƒ½ä¸æ‚¨çš„åŸå§‹æ„å›¾æœ‰ç»†å¾®å·®åˆ«
    # å¦‚æœæ‚¨çš„å¸ƒå±€ç®—æ³•ä¸­å·²ç»æœ‰è¿™äº›æ¦‚å¿µï¼Œè¯·æ›¿æ¢æˆæ­£ç¡®çš„ç‰ˆæœ¬
    undirected_graph = defaultdict(set)
    for u, vs in successors.items():
        for v in vs:
            undirected_graph[u].add(v)
            undirected_graph[v].add(u)
    
    # ç®€å•åœ°å°†æ‰€æœ‰èŠ‚ç‚¹è§†ä¸ºä¸€ä¸ªå¤§é›†ç¾¤
    # å¦‚æœæ‚¨çš„ç®—æ³•æ˜¯å¤šé›†ç¾¤çš„ï¼Œè¯·ä½¿ç”¨æ­£ç¡®çš„é›†ç¾¤åˆ’åˆ†é€»è¾‘
    all_node_ids = list(final_positions.keys())
    simple_clusters = [all_node_ids] if all_node_ids else []

    final_positions = _fishschool_local_swaps(predecessors, successors, undirected_graph, simple_clusters, final_positions, max_pass=3)
    print("   å±€éƒ¨äº¤æ¢ä¼˜åŒ–å®Œæˆã€‚")

    return final_positions

# --- ä¸»æ‰§è¡Œæµç¨‹ (ç”¨äºç‹¬ç«‹è¿è¡Œ) ---
if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨ç»ˆæå¸ƒå±€ç®—æ³• (ç‹¬ç«‹è¿è¡Œæ¨¡å¼)...")
    
    try:
        with open(INPUT_FILENAME, 'r', encoding='utf-8') as f:
            full_data = json.load(f)
        chip_graph_str = next(md['stringValue'] for md in full_data['saveObjectContainers'][0]['saveObjects']['saveMetaDatas'] if md['key'] == 'chip_graph')
        chip_nodes = json.loads(chip_graph_str).get('Nodes', [])
    except Exception as e:
        print(f"âŒ é”™è¯¯: æ— æ³•åœ¨ '{INPUT_FILENAME}' ä¸­è¯»å–æˆ–æ‰¾åˆ°èŠ¯ç‰‡æ•°æ®ã€‚è¯¦æƒ…: {e}")
        exit()

    print(f"âœ… æ‰¾åˆ° {len(chip_nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
    
    # è°ƒç”¨æ–°çš„æ ¸å¿ƒå‡½æ•°
    final_positions = run_layout_engine(chip_nodes)

    print("5. ä½¿ç”¨æ–°åæ ‡æ›´æ–°JSONæ–‡ä»¶...")
    if find_and_update_chip_graph(full_data, final_positions):
        with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:
            json.dump(full_data, f, indent=4) # ç‹¬ç«‹è¿è¡Œæ—¶ä½¿ç”¨ indent=4 æ–¹ä¾¿æŸ¥çœ‹
        print(f"\nğŸ‰ æˆåŠŸï¼å·²ç”Ÿæˆç»ˆæå¸ƒå±€æ–‡ä»¶: '{OUTPUT_FILENAME}'")
    else:
        print("âŒ è‡´å‘½é”™è¯¯: æ— æ³•åœ¨JSONæ–‡ä»¶ä¸­æ‰¾åˆ° 'chip_graph' ä»¥è¿›è¡Œæ›´æ–°ã€‚")
